{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2940fa2-aaca-4622-8b9a-6115c657f4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Analytics -1. Extract Sample document and apply following document preprocessing methods: \n",
    "# Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization. \n",
    "# 2. Create representation of document by calculating Term Frequency and Inverse Document Frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "659a1e69-e4ce-4876-81ad-d3fbc707006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import libraries\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50ef3d79-48ff-4a68-813c-58e3defefc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b1c66d2-26d8-4f29-957e-3caf7a841d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document:\n",
      " Natural Language Processing is a branch of artificial intelligence that helps computers understand, interpret and manipulate human language.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Sample document\n",
    "doc = \"Natural Language Processing is a branch of artificial intelligence that helps computers understand, interpret and manipulate human language.\"\n",
    "print(\"Original Document:\\n\", doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94d078ef-b850-4ae5-8fe3-9ff98b168d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization:\n",
      " ['Natural', 'Language', 'Processing', 'is', 'a', 'branch', 'of', 'artificial', 'intelligence', 'that', 'helps', 'computers', 'understand', ',', 'interpret', 'and', 'manipulate', 'human', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Tokenization\n",
    "tokens = word_tokenize(doc)\n",
    "print(\"\\nTokenization:\\n\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23779ebf-433e-4ea5-b321-2711fd8dcbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POS Tagging:\n",
      " [('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('branch', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('that', 'WDT'), ('helps', 'VBZ'), ('computers', 'NNS'), ('understand', 'VBP'), (',', ','), ('interpret', 'JJ'), ('and', 'CC'), ('manipulate', 'JJ'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: POS Tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(\"\\nPOS Tagging:\\n\", pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81f52918-a98f-49f0-afbd-8133f052ea4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Stop Words Removal:\n",
      " ['Natural', 'Language', 'Processing', 'branch', 'artificial', 'intelligence', 'helps', 'computers', 'understand', 'interpret', 'manipulate', 'human', 'language']\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Stop Words Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens_no_stop = [word for word in tokens if word.lower() not in stop_words and word.isalpha()]\n",
    "print(\"\\nAfter Stop Words Removal:\\n\", tokens_no_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6fafb4c-33d9-42c3-850f-b588c1c58c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Stemming:\n",
      " ['natur', 'languag', 'process', 'branch', 'artifici', 'intellig', 'help', 'comput', 'understand', 'interpret', 'manipul', 'human', 'languag']\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in tokens_no_stop]\n",
    "print(\"\\nAfter Stemming:\\n\", stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d5b4f85-e7f5-49d2-bafc-e95245a84f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Lemmatization:\n",
      " ['natural', 'language', 'processing', 'branch', 'artificial', 'intelligence', 'help', 'computer', 'understand', 'interpret', 'manipulate', 'human', 'language']\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word.lower()) for word in tokens_no_stop]\n",
    "print(\"\\nAfter Lemmatization:\\n\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e44f359c-074c-4fcf-973e-f98d642804f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: TF-IDF Vectorization\n",
    "# Let's add a second simple document for context\n",
    "docs = [\n",
    "    doc,\n",
    "    \"Machine learning is an important part of artificial intelligence that enables computers to learn from data.\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5a7ce23-f47e-4cc3-9c29-7e823174bcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Representation:\n",
      "\n",
      "         an       and  artificial    branch  computers      data   enables  \\\n",
      "0  0.000000  0.249708    0.177669  0.249708   0.177669  0.000000  0.000000   \n",
      "1  0.276951  0.000000    0.197053  0.000000   0.197053  0.276951  0.276951   \n",
      "\n",
      "       from     helps     human  ...  learning   machine  manipulate  \\\n",
      "0  0.000000  0.249708  0.249708  ...  0.000000  0.000000    0.249708   \n",
      "1  0.276951  0.000000  0.000000  ...  0.276951  0.276951    0.000000   \n",
      "\n",
      "    natural        of      part  processing      that        to  understand  \n",
      "0  0.249708  0.177669  0.000000    0.249708  0.177669  0.000000    0.249708  \n",
      "1  0.000000  0.197053  0.276951    0.000000  0.197053  0.276951    0.000000  \n",
      "\n",
      "[2 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert to DataFrame\n",
    "import pandas as pd\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(\"\\nTF-IDF Representation:\\n\")\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af347f57-ec5b-4d16-ab2c-019f39fcb7ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
